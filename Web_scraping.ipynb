{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df9GpukT49lp"
      },
      "source": [
        "# **Crawling some of the gov.tn websites**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFIKhLndIA6d"
      },
      "source": [
        "## **santetunisie.rns.tn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWgLh2Ov4lzS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Get the HTML content of the target page\n",
        "url = \"http://www.santetunisie.rns.tn/fr/questions-frequentes\"\n",
        "html = requests.get(url).content\n",
        "\n",
        "# Parse the HTML content using Beautiful Soup\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Find all the question and answer pairs using the appropriate HTML tags and attributes\n",
        "qa_pairs = soup.find_all(class_=\"jp-accordion-item\")\n",
        "\n",
        "# Extract the text of each question and answer and store them in a list or dictionary\n",
        "qas = []\n",
        "for pair in qa_pairs:\n",
        "    question = pair.find(class_ =\"ui-accordion-header ui-helper-reset ui-state-default ui-corner-all\").get_text()\n",
        "    answer = pair.find(class_ =\"ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom\").get_text()\n",
        "    qas.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "# Print the extracted question and answer pairs\n",
        "for qa in qas:\n",
        "    print(\"Question: \", qa[\"question\"])\n",
        "    print(\"Answer: \", qa[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77iUE7sZ6Wzd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "questions_list=[]\n",
        "answers_list=[]\n",
        "\n",
        "page_url = \"http://www.santetunisie.rns.tn/fr/questions-frequentes\"\n",
        "response = requests.get(page_url)\n",
        "html_content = response.content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "\n",
        "# Scrape questions and answers\n",
        "questions = soup.find_all(attrs={\"class\": \"ui-accordion-header ui-helper-reset ui-state-default ui-corner-all\"})\n",
        "answers = soup.find_all(attrs={\"class\": \"ui-accordion-content ui-helper-reset ui-widget-content ui-corner-bottom\"})\n",
        "\n",
        "# Remove html tags\n",
        "questions = [question.text.strip() for question in questions]\n",
        "answers = [answer.text.strip() for answer in answers]\n",
        "\n",
        "# Create Q&A lists\n",
        "questions_list.append(questions)\n",
        "answers_list.append(answers)\n",
        "\n",
        "# Flatten the lists\n",
        "questions_list = [item for sublist in questions_list for item in sublist]\n",
        "answers_list = [item for sublist in answers_list for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb2Z0lel7Bc4"
      },
      "outputs": [],
      "source": [
        "QA_dict = {'questions':questions_list, 'answers':answers_list}\n",
        "QA1 = pd.DataFrame(QA_dict)\n",
        "QA1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAnZpdQrIINS"
      },
      "source": [
        "## **finances.gov.tn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpZHBCz45nz8"
      },
      "outputs": [],
      "source": [
        "url = 'http://www.finances.gov.tn/fr/faq?body_value=&field_themef_target_id=All&page=0'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "# Find the last page button and extract the URL\n",
        "last_page_url  = soup.find('a', attrs={\"title\": \"Aller à la dernière page\"}).get('href')\n",
        "\n",
        "# Extract the maximum page number from the last page URL\n",
        "max_pages = int(last_page_url.split('=')[-1]) + 1 # +1 (page numeration starts from 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP4k58xd5uQo"
      },
      "outputs": [],
      "source": [
        "questions_list=[]\n",
        "answers_list=[]\n",
        "\n",
        "for page in range(max_pages):\n",
        "\n",
        "    page_url = f'http://www.finances.gov.tn/fr/faq?body_value=&field_themef_target_id=All&page={page}'\n",
        "    response = requests.get(page_url)\n",
        "    html_content = response.content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "\n",
        "    # Scrape questions and answers\n",
        "    questions = soup.find_all(attrs={\"class\": \"question\"})\n",
        "    answers = soup.find_all(attrs={\"class\": \"reponse\"})\n",
        "\n",
        "    # Remove html tags\n",
        "    questions = [question.text.strip() for question in questions]\n",
        "    answers = [answer.text.strip() for answer in answers]\n",
        "\n",
        "    # Create Q&A lists\n",
        "    questions_list.append(questions)\n",
        "    answers_list.append(answers)\n",
        "\n",
        "# Flatten the lists\n",
        "questions_list = [item for sublist in questions_list for item in sublist]\n",
        "answers_list = [item for sublist in answers_list for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_b0UKwh5wd5"
      },
      "outputs": [],
      "source": [
        "QA_dict = {'questions':questions_list, 'answers':answers_list}\n",
        "QA2 = pd.DataFrame(QA_dict)\n",
        "QA2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk0mRtfh51QS"
      },
      "source": [
        "## **environnement.gov.tn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3eV2q3f55Qj"
      },
      "outputs": [],
      "source": [
        "url = 'http://www.environnement.gov.tn/faq'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "questions = soup.find_all('h3', attrs={\"class\": \"toggleTrigger\"})\n",
        "answers = soup.find_all('div', attrs={\"class\": \"jpfaqAnswer\"})\n",
        "\n",
        "# Remove html tags\n",
        "questions_list = [question.text.strip() for question in questions]\n",
        "answers_list = [answer.text.strip() for answer in answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAvfwEVU56ry"
      },
      "outputs": [],
      "source": [
        "QA_dict = {'questions':questions_list, 'answers':answers_list}\n",
        "QA3 = pd.DataFrame(QA_dict)\n",
        "QA3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBEMKFMo5-0D"
      },
      "source": [
        "##**tunisie.gov.tn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ns4fpKS5-f1"
      },
      "outputs": [],
      "source": [
        "url = 'http://fr.tunisie.gov.tn/7-foire-aux-questions.htm?idtf=7'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "questions = soup.find_all(attrs={\"class\": \"panel-title\"})\n",
        "answers = soup.find_all('p')\n",
        "\n",
        "# Remove html tags\n",
        "questions_list = [question.text.strip() for question in questions]\n",
        "answers_list = [answer.text.strip() for answer in answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPSjAdXt6BZy"
      },
      "outputs": [],
      "source": [
        "QA_dict = {'questions':questions_list, 'answers':answers_list}\n",
        "QA4 = pd.DataFrame(QA_dict)\n",
        "QA4.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVG6BQQI6EWa"
      },
      "source": [
        "## **social.gov.tn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tMcoKdH6Gbr"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.social.gov.tn/fr/faq?keyword=&service=All&page=0'\n",
        "response = requests.get(url, verify=False)\n",
        "soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "# Find the last page button and extract the URL\n",
        "last_page_url  = soup.find('a', attrs={\"title\": \"Go to last page\"}).get('href')\n",
        "\n",
        "# Extract the maximum page number from the last page URL\n",
        "max_pages = int(last_page_url.split('=')[-1]) + 1 # +1 (page numeration starts from 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LuMVQJi6IjJ"
      },
      "outputs": [],
      "source": [
        "questions_list=[]\n",
        "answers_list=[]\n",
        "\n",
        "for page in range(max_pages):\n",
        "\n",
        "    page_url = f'https://www.social.gov.tn/fr/faq?keyword=&service=All&page={page}'\n",
        "    response = requests.get(page_url, verify=False)\n",
        "    html_content = response.content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "\n",
        "    # Scrape questions and answers\n",
        "    questions = soup.find_all('h4', attrs={\"class\": \"field-content c-font-14\"})\n",
        "    answers = soup.find_all('div', attrs={\"class\": \"views-field views-field-field-reponse\"})\n",
        "\n",
        "    # Remove html tags\n",
        "    questions = [question.text.strip() for question in questions]\n",
        "    answers = [answer.text.replace(u'\\xa0', u' ').replace(\"\\'\", \"'\").strip() for answer in answers]\n",
        "\n",
        "    # Create Q&A lists\n",
        "    questions_list.append(questions)\n",
        "    answers_list.append(answers)\n",
        "\n",
        "# Flatten the lists\n",
        "questions_list = [item for sublist in questions_list for item in sublist]\n",
        "answers_list = [item for sublist in answers_list for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcc-DZwJ6KS6"
      },
      "outputs": [],
      "source": [
        "QA_dict = {'questions':questions_list, 'answers':answers_list}\n",
        "QA5 = pd.DataFrame(QA_dict)\n",
        "QA5.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yVMSQAOKdFL"
      },
      "source": [
        "##**Final output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcS3mwD-9c9I"
      },
      "outputs": [],
      "source": [
        "final_df = pd.concat([QA1, QA2, QA3,QA4, QA5], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyBpJtt-9ypG"
      },
      "outputs": [],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLW09QRf-11R"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv('data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9phms8Ds5xN8"
      },
      "source": [
        "# **Training, Fine-tuning and evaluating the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEfcOJrj2_B5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6CIpPde_TaY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
        "import pandas as pd\n",
        "\n",
        "# load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# load the training data from the CSV file\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# convert the data to BERT-compatible format\n",
        "inputs = []\n",
        "for i in range(len(data)):\n",
        "    context = data.iloc[i]['answers']\n",
        "    question = data.iloc[i]['questions']\n",
        "    input_text = f\"{question} [SEP] {context}\"\n",
        "    tokenized = tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        pad_to_max_length=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    inputs.append(tokenized)\n",
        "\n",
        "# convert the inputs to PyTorch tensors and create a DataLoader\n",
        "input_ids = torch.cat([i['input_ids'] for i in inputs], dim=0)\n",
        "attention_mask = torch.cat([i['attention_mask'] for i in inputs], dim=0)\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "for i in range(len(data)):\n",
        "    context = data.iloc[i]['answers']\n",
        "    question = data.iloc[i]['questions']\n",
        "    start_pos = context.find(question)\n",
        "    end_pos = start_pos + len(question)\n",
        "    start_positions.append(start_pos)\n",
        "    end_positions.append(end_pos)\n",
        "start_positions = torch.tensor(start_positions)\n",
        "end_positions = torch.tensor(end_positions)\n",
        "dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, start_positions, end_positions)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "\n",
        "# fine-tune the BERT model for question answering\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, start_positions, end_positions = batch\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# save the fine-tuned model\n",
        "model.save_pretrained('qa_model')\n",
        "tokenizer.save_pretrained('qa_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whs5UOSBGd8t"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"qa_model\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"qa_model\")\n",
        "\n",
        "# List of questions to ask the model\n",
        "questions = [\n",
        "    \"C'est quoi la grippe A(H1N1)?\",\n",
        "    \"Who is the health minister of Tunisia ?\",\n",
        "    \"Comment vérifie l'administration fiscale\"\n",
        "]\n",
        "\n",
        "# Loop through each question and ask the model\n",
        "for question in questions:\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(question, padding='max_length', truncation=True, max_length=64, return_tensors='pt')\n",
        "\n",
        "    # Perform the forward pass\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Extract the start and end indices of the answer\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    # Decode the answer and print it\n",
        "    answer_tokens = inputs['input_ids'][0][answer_start:answer_end]\n",
        "    answer = tokenizer.decode(answer_tokens)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}