{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7uSw3UQxfL-"
      },
      "source": [
        "## **Checking the python'version**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:47:33.825150Z",
          "iopub.status.busy": "2023-04-09T16:47:33.824509Z",
          "iopub.status.idle": "2023-04-09T16:47:34.870278Z",
          "shell.execute_reply": "2023-04-09T16:47:34.869060Z",
          "shell.execute_reply.started": "2023-04-09T16:47:33.825114Z"
        },
        "id": "r4L2rilLw19E",
        "outputId": "2aca872e-ef84-4240-b0f0-443076b00d4e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWBQgtQbyMai"
      },
      "source": [
        "## **Creating a new conda environment and installing the Anaconda distribution of Python version 3.9 into it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:47:34.872930Z",
          "iopub.status.busy": "2023-04-09T16:47:34.872512Z",
          "iopub.status.idle": "2023-04-09T16:47:36.998579Z",
          "shell.execute_reply": "2023-04-09T16:47:36.997487Z",
          "shell.execute_reply.started": "2023-04-09T16:47:34.872891Z"
        },
        "id": "kefVC0Ocw19F",
        "outputId": "2c3c464f-4a32-4491-eef4-52f52b541d31",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/bin/conda\", line 14, in <module>\n",
            "    from conda.cli import main\n",
            "ModuleNotFoundError: No module named 'conda'\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/bin/conda\", line 14, in <module>\n",
            "    from conda.cli import main\n",
            "ModuleNotFoundError: No module named 'conda'\n"
          ]
        }
      ],
      "source": [
        "!conda create -n py39 python=3.9 anaconda --yes\n",
        "!source /opt/conda/bin/activate py39 && conda install -c py39 python -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:47:37.002220Z",
          "iopub.status.busy": "2023-04-09T16:47:37.001827Z",
          "iopub.status.idle": "2023-04-09T16:47:39.078427Z",
          "shell.execute_reply": "2023-04-09T16:47:39.077173Z",
          "shell.execute_reply.started": "2023-04-09T16:47:37.002180Z"
        },
        "id": "_N4t_-3ow19F",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!sudo rm /opt/conda/bin/python3\n",
        "!sudo ln -sf /opt/conda/envs/py39/bin/python3 /opt/conda/bin/python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:47:39.082554Z",
          "iopub.status.busy": "2023-04-09T16:47:39.082153Z",
          "iopub.status.idle": "2023-04-09T16:47:41.103536Z",
          "shell.execute_reply": "2023-04-09T16:47:41.102278Z",
          "shell.execute_reply.started": "2023-04-09T16:47:39.082517Z"
        },
        "id": "-1SnoXzVw19G",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!sudo rm /opt/conda/bin/python3.7\n",
        "!sudo ln -sf /opt/conda/envs/py39/bin/python3 /opt/conda/bin/python3.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:47:41.106925Z",
          "iopub.status.busy": "2023-04-09T16:47:41.106505Z",
          "iopub.status.idle": "2023-04-09T16:47:43.095868Z",
          "shell.execute_reply": "2023-04-09T16:47:43.094535Z",
          "shell.execute_reply.started": "2023-04-09T16:47:41.106884Z"
        },
        "id": "GwGWgPMvw19G",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!sudo rm /opt/conda/bin/python\n",
        "!sudo ln -s /opt/conda/envs/py39/bin/python3 /opt/conda/bin/python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:47:43.099493Z",
          "iopub.status.busy": "2023-04-09T16:47:43.098498Z",
          "iopub.status.idle": "2023-04-09T16:47:44.092259Z",
          "shell.execute_reply": "2023-04-09T16:47:44.090958Z",
          "shell.execute_reply.started": "2023-04-09T16:47:43.099451Z"
        },
        "id": "LMz5jyL4w19G",
        "outputId": "ecbf8f8f-1ff9-4a91-c7ba-ecbdd428be18",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anXdTstryhVx"
      },
      "source": [
        "## **Installing the requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:47:44.094168Z",
          "iopub.status.busy": "2023-04-09T16:47:44.093786Z",
          "iopub.status.idle": "2023-04-09T16:48:01.082589Z",
          "shell.execute_reply": "2023-04-09T16:48:01.081461Z",
          "shell.execute_reply.started": "2023-04-09T16:47:44.094124Z"
        },
        "id": "V39JdQkDw19H",
        "outputId": "be1c230b-2b94-4ee8-9101-efb5425839ac",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/conda/envs/py39/lib/python3.9/site-packages (4.27.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /opt/conda/envs/py39/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: bitsandbytes in /opt/conda/envs/py39/lib/python3.9/site-packages (0.37.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: accelerate in /opt/conda/envs/py39/lib/python3.9/site-packages (0.18.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from accelerate) (2.0.0.post200)\n",
            "Requirement already satisfied: psutil in /opt/conda/envs/py39/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/py39/lib/python3.9/site-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/envs/py39/lib/python3.9/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/envs/py39/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/envs/py39/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/envs/py39/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /opt/conda/envs/py39/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/envs/py39/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py39/lib/python3.9/site-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCd4GZbexV5R"
      },
      "source": [
        "## **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:01.084439Z",
          "iopub.status.busy": "2023-04-09T16:48:01.084083Z",
          "iopub.status.idle": "2023-04-09T16:48:03.941898Z",
          "shell.execute_reply": "2023-04-09T16:48:03.938343Z",
          "shell.execute_reply.started": "2023-04-09T16:48:01.084403Z"
        },
        "id": "pIGgBpv3w19H",
        "outputId": "ca55b520-92da-4391-954e-f180c0de52ae",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 113\n",
            "CUDA SETUP: Loading binary /opt/conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import init_empty_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaCkDjyNxwgU"
      },
      "source": [
        "## **Traversing through the directory path '/kaggle/input' and printing out the names of all files within the directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:03.944080Z",
          "iopub.status.busy": "2023-04-09T16:48:03.943258Z",
          "iopub.status.idle": "2023-04-09T16:48:03.956099Z",
          "shell.execute_reply": "2023-04-09T16:48:03.954932Z",
          "shell.execute_reply.started": "2023-04-09T16:48:03.944043Z"
        },
        "id": "PPKCXZIdw19B",
        "outputId": "cfdb2cb6-2a3b-4726-d671-55d7b129e6f3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/nlu-data/data.csv\n"
          ]
        }
      ],
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vltA3PY1zD68"
      },
      "source": [
        "## **FrozenBNBLinear represents a linear transformation layer that uses quantized weights and biases**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:03.958120Z",
          "iopub.status.busy": "2023-04-09T16:48:03.957692Z",
          "iopub.status.idle": "2023-04-09T16:48:03.967541Z",
          "shell.execute_reply": "2023-04-09T16:48:03.966440Z",
          "shell.execute_reply.started": "2023-04-09T16:48:03.958085Z"
        },
        "id": "KWcrdx1dzhLC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
        "        if self.adapter:\n",
        "            output_cloned = torch.clone(output + self.adapter(input))\n",
        "            return output_cloned\n",
        "        else :\n",
        "            return output\n",
        "\n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TVzHcE2zh_Q"
      },
      "source": [
        "## **DequantizeAndLinear performs the dequantization step for the FrozenBNBLinear layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:03.969647Z",
          "iopub.status.busy": "2023-04-09T16:48:03.968826Z",
          "iopub.status.idle": "2023-04-09T16:48:03.980335Z",
          "shell.execute_reply": "2023-04-09T16:48:03.979335Z",
          "shell.execute_reply.started": "2023-04-09T16:48:03.969612Z"
        },
        "id": "qd1QzNkxzsRi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DequantizeAndLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        "\n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlnWJwh3zuzC"
      },
      "source": [
        "## **FrozenBNBEmbedding represents an embedding layer. In fact, the forward method first dequantizes the quantized weights and then applies the embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:03.982488Z",
          "iopub.status.busy": "2023-04-09T16:48:03.981922Z",
          "iopub.status.idle": "2023-04-09T16:48:03.993183Z",
          "shell.execute_reply": "2023-04-09T16:48:03.992311Z",
          "shell.execute_reply.started": "2023-04-09T16:48:03.982454Z"
        },
        "id": "zskeQhscz-r-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "\n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "\n",
        "            output_cloned = torch.clone(output + self.adapter(input))\n",
        "            return output_cloned\n",
        "        else :\n",
        "            return output\n",
        "\n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afw9xrp8z_Vf"
      },
      "source": [
        "## **quantize_blockise_lowmemory is a function that quantizes a PyTorch tensor using blockwise quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:03.998657Z",
          "iopub.status.busy": "2023-04-09T16:48:03.998413Z",
          "iopub.status.idle": "2023-04-09T16:48:04.007967Z",
          "shell.execute_reply": "2023-04-09T16:48:04.007101Z",
          "shell.execute_reply.started": "2023-04-09T16:48:03.998635Z"
        },
        "id": "xRSTgmrn0JbE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        "\n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HOY_00Y0LzY"
      },
      "source": [
        "## **convert_to_int8 is a function that converts all linear and embedding layers in a given PyTorch model to 8-bit with optional adapters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:04.009695Z",
          "iopub.status.busy": "2023-04-09T16:48:04.009367Z",
          "iopub.status.idle": "2023-04-09T16:48:04.019969Z",
          "shell.execute_reply": "2023-04-09T16:48:04.018259Z",
          "shell.execute_reply.started": "2023-04-09T16:48:04.009663Z"
        },
        "id": "jl6mIm25w19I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf4ko3vT0ZTO"
      },
      "source": [
        "## **GPTJBlock represents a single block of the GPT-J transformer model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:04.023202Z",
          "iopub.status.busy": "2023-04-09T16:48:04.022432Z",
          "iopub.status.idle": "2023-04-09T16:48:04.200137Z",
          "shell.execute_reply": "2023-04-09T16:48:04.199177Z",
          "shell.execute_reply.started": "2023-04-09T16:48:04.023103Z"
        },
        "id": "d8bkAaTs0eUk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PC93oYG0eqI"
      },
      "source": [
        "## **GPTJModel that inherits from the GPTJModel class provided by the transformers library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:04.202033Z",
          "iopub.status.busy": "2023-04-09T16:48:04.201648Z",
          "iopub.status.idle": "2023-04-09T16:48:04.209391Z",
          "shell.execute_reply": "2023-04-09T16:48:04.207688Z",
          "shell.execute_reply.started": "2023-04-09T16:48:04.201997Z"
        },
        "id": "qCG79ju301t4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpfKlo0D07Qe"
      },
      "source": [
        "## **GPTJForCausalLM is a class created based on an existing class GPTJForCausalLM in the transformers library and it is used for generating text using a GPT-J model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:04.211538Z",
          "iopub.status.busy": "2023-04-09T16:48:04.211173Z",
          "iopub.status.idle": "2023-04-09T16:48:04.219343Z",
          "shell.execute_reply": "2023-04-09T16:48:04.218419Z",
          "shell.execute_reply.started": "2023-04-09T16:48:04.211503Z"
        },
        "id": "1XhLlNmVw19J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHw8RikP1TJ-"
      },
      "source": [
        "##**Importing the configuration and tokenizer for the EleutherAI GPT-J 6B model using the Hugging Face Transformers library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:04.221070Z",
          "iopub.status.busy": "2023-04-09T16:48:04.220564Z",
          "iopub.status.idle": "2023-04-09T16:48:04.953042Z",
          "shell.execute_reply": "2023-04-09T16:48:04.951956Z",
          "shell.execute_reply.started": "2023-04-09T16:48:04.221038Z"
        },
        "id": "MzixJamOw19J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQPJKY2W1jJN"
      },
      "source": [
        "## **Initializing a GPTJForCausalLM model that has been fine-tuned on the EleutherAI GPT-J 6B model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:48:04.956342Z",
          "iopub.status.busy": "2023-04-09T16:48:04.954395Z",
          "iopub.status.idle": "2023-04-09T16:49:04.159913Z",
          "shell.execute_reply": "2023-04-09T16:49:04.158894Z",
          "shell.execute_reply.started": "2023-04-09T16:48:04.956290Z"
        },
        "id": "2uXYqbd-w19K",
        "outputId": "e13ab8a8-0cb8-4959-85d6-d959e17c86cd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "lm_head Linear(in_features=4096, out_features=50400, bias=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPTJForCausalLM(\n",
              "  (transformer): GPTJModel(\n",
              "    (wte): FrozenBNBEmbedding(50400, 4096)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-27): 28 x GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
              "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): FrozenBNBLinear(4096, 50400)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n",
        "\n",
        "# checking if a GPU is available and set the device accordingly\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "\n",
        "gpt.to(device) #move it to the specified device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLgQSp-I2ADK"
      },
      "source": [
        "## **Loading data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:04.165304Z",
          "iopub.status.busy": "2023-04-09T16:49:04.164954Z",
          "iopub.status.idle": "2023-04-09T16:49:05.964689Z",
          "shell.execute_reply": "2023-04-09T16:49:05.963737Z",
          "shell.execute_reply.started": "2023-04-09T16:49:04.165258Z"
        },
        "id": "R9p1yBQ1w19K",
        "outputId": "14ad680e-486f-455c-e882-f5259e8fbc80",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[questions]:Comment les gens contractent-ils le virus ?\n",
            "[answers]:Le virus se transmet d’homme à homme aussi facilement que celui de la grippe saisonnière normale, lorsque des personnes infectées toussent ou éternuent et que les gouttelettes infectées sont inhalées ou contaminent les mains ou des surfaces.\n",
            "Pour prévenir la propagation de la maladie, les personnes malades doivent se couvrir le nez et la bouche lorsqu’elles toussent ou éternuent, rester chez elles si elles ne se sentent pas bien, se laver les mains régulièrement et se tenir autant que possible à l’écart des personnes bien portantes.\n",
            "On n’a connaissance d’aucun cas d’infection humaine consécutive à une exposition à des porcs ou à d’autres animaux.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/kaggle/input/nlu-data/data.csv')\n",
        "\n",
        "\n",
        "data['sentence'] = '[questions]:'+data['questions']+'\\n[answers]:'+data['answers']\n",
        "data=data['sentence']\n",
        "print(data.iloc[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIsGbwEt2TOv"
      },
      "source": [
        "## **Split the data into train and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:05.966642Z",
          "iopub.status.busy": "2023-04-09T16:49:05.966264Z",
          "iopub.status.idle": "2023-04-09T16:49:05.997803Z",
          "shell.execute_reply": "2023-04-09T16:49:05.996958Z",
          "shell.execute_reply.started": "2023-04-09T16:49:05.966607Z"
        },
        "id": "JP0aMK_hw19K",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(data, test_size=0.01)\n",
        "train.to_csv('/train.csv', index=False)\n",
        "test.to_csv('/test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a585c8d88aee40fb8981936a2b31776d",
            "50fc21b1d6ec445c8cac28dd69a18d9e",
            "2e06f7f2f7d94293bd8b5752d1edaab5",
            "d402fe83fbce47b794e09017584355c0",
            "26829254d5ca4a04b83630c0870d6543",
            "a3a752e8bc22494888ec4e8a36e383e0",
            "e59aabc9aff84fe09eef342d1f4e4765",
            "ca57868fe0e14fb9913d334f0cfe71a1"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:05.999795Z",
          "iopub.status.busy": "2023-04-09T16:49:05.999373Z",
          "iopub.status.idle": "2023-04-09T16:49:07.367226Z",
          "shell.execute_reply": "2023-04-09T16:49:07.366245Z",
          "shell.execute_reply.started": "2023-04-09T16:49:05.999757Z"
        },
        "id": "tfCly6kew19K",
        "outputId": "4361577b-00e0-4eca-a239-28d638476ac1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-99f953c4a20e12da/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d402fe83fbce47b794e09017584355c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26829254d5ca4a04b83630c0870d6543",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3a752e8bc22494888ec4e8a36e383e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e59aabc9aff84fe09eef342d1f4e4765",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-99f953c4a20e12da/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca57868fe0e14fb9913d334f0cfe71a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files={'train': '/train.csv',\n",
        "                                              'test': '/test.csv'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:07.369902Z",
          "iopub.status.busy": "2023-04-09T16:49:07.368906Z",
          "iopub.status.idle": "2023-04-09T16:49:07.380747Z",
          "shell.execute_reply": "2023-04-09T16:49:07.379389Z",
          "shell.execute_reply.started": "2023-04-09T16:49:07.369860Z"
        },
        "id": "GqUSfZJtw19L",
        "outputId": "be42bc2d-5534-4049-f6da-4cc78c5fd791",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence'],\n",
              "        num_rows: 144\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence'],\n",
              "        num_rows: 2\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:07.383621Z",
          "iopub.status.busy": "2023-04-09T16:49:07.382831Z",
          "iopub.status.idle": "2023-04-09T16:49:07.392156Z",
          "shell.execute_reply": "2023-04-09T16:49:07.390973Z",
          "shell.execute_reply.started": "2023-04-09T16:49:07.383584Z"
        },
        "id": "DEhzzX-Nw19L",
        "outputId": "0c7fb9cb-4042-4dd1-9622-4413f5133471",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device # first available CUDA device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3072_hIv3BMf"
      },
      "source": [
        "## **Setting the padding token of the tokenizer to be the end-of-sequence token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:07.394724Z",
          "iopub.status.busy": "2023-04-09T16:49:07.394048Z",
          "iopub.status.idle": "2023-04-09T16:49:07.414793Z",
          "shell.execute_reply": "2023-04-09T16:49:07.413505Z",
          "shell.execute_reply.started": "2023-04-09T16:49:07.394691Z"
        },
        "id": "FdbuQp-Mw19L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlQC6LP83X9n"
      },
      "source": [
        "## **Preparing the input data for feeding into the GPT-J model by converting the text into tokens that can be used as input to the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a62a38848f2f473bafacab86fbb022f1",
            "fe7984238a984d8c83e81e559705401e",
            "aff2a6f2ebbd46e3949eac3d3283515f",
            "5bad923b66b84831a006376ceff74981"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:07.417413Z",
          "iopub.status.busy": "2023-04-09T16:49:07.416685Z",
          "iopub.status.idle": "2023-04-09T16:49:07.702319Z",
          "shell.execute_reply": "2023-04-09T16:49:07.701411Z",
          "shell.execute_reply.started": "2023-04-09T16:49:07.417377Z"
        },
        "id": "6tpwR4wiw19L",
        "outputId": "2d980aee-a45d-473f-b809-10aff4e86221",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aff2a6f2ebbd46e3949eac3d3283515f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bad923b66b84831a006376ceff74981",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=True, truncation=True, max_length= 128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\"])\n",
        "tokenized_datasets.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx33HICO3yRy"
      },
      "source": [
        "## **Creating a PyTorch DataLoader object named train_dataloader from a tokenized dataset named full_train_dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:07.704465Z",
          "iopub.status.busy": "2023-04-09T16:49:07.703687Z",
          "iopub.status.idle": "2023-04-09T16:49:07.712867Z",
          "shell.execute_reply": "2023-04-09T16:49:07.711658Z",
          "shell.execute_reply.started": "2023-04-09T16:49:07.704430Z"
        },
        "id": "aorUHteBw19L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "train_dataloader = DataLoader(full_train_dataset, shuffle=False, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK8nnQSa4AHR"
      },
      "source": [
        "## **Adding adapter layers to a given GPTJ model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:07.715080Z",
          "iopub.status.busy": "2023-04-09T16:49:07.714149Z",
          "iopub.status.idle": "2023-04-09T16:49:07.893281Z",
          "shell.execute_reply": "2023-04-09T16:49:07.892345Z",
          "shell.execute_reply.started": "2023-04-09T16:49:07.715046Z"
        },
        "id": "HcusvPxsw19M",
        "outputId": "98d55fcd-b167-49bd-e4d1-de49d5d67b03",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding adapter to transformer.wte\n",
            "Initializing transformer.wte\n",
            "Adding adapter to transformer.h.0.attn.k_proj\n",
            "Initializing transformer.h.0.attn.k_proj\n",
            "Adding adapter to transformer.h.0.attn.v_proj\n",
            "Initializing transformer.h.0.attn.v_proj\n",
            "Adding adapter to transformer.h.0.attn.q_proj\n",
            "Initializing transformer.h.0.attn.q_proj\n",
            "Adding adapter to transformer.h.0.attn.out_proj\n",
            "Initializing transformer.h.0.attn.out_proj\n",
            "Adding adapter to transformer.h.0.mlp.fc_in\n",
            "Initializing transformer.h.0.mlp.fc_in\n",
            "Adding adapter to transformer.h.0.mlp.fc_out\n",
            "Initializing transformer.h.0.mlp.fc_out\n",
            "Adding adapter to transformer.h.1.attn.k_proj\n",
            "Initializing transformer.h.1.attn.k_proj\n",
            "Adding adapter to transformer.h.1.attn.v_proj\n",
            "Initializing transformer.h.1.attn.v_proj\n",
            "Adding adapter to transformer.h.1.attn.q_proj\n",
            "Initializing transformer.h.1.attn.q_proj\n",
            "Adding adapter to transformer.h.1.attn.out_proj\n",
            "Initializing transformer.h.1.attn.out_proj\n",
            "Adding adapter to transformer.h.1.mlp.fc_in\n",
            "Initializing transformer.h.1.mlp.fc_in\n",
            "Adding adapter to transformer.h.1.mlp.fc_out\n",
            "Initializing transformer.h.1.mlp.fc_out\n",
            "Adding adapter to transformer.h.2.attn.k_proj\n",
            "Initializing transformer.h.2.attn.k_proj\n",
            "Adding adapter to transformer.h.2.attn.v_proj\n",
            "Initializing transformer.h.2.attn.v_proj\n",
            "Adding adapter to transformer.h.2.attn.q_proj\n",
            "Initializing transformer.h.2.attn.q_proj\n",
            "Adding adapter to transformer.h.2.attn.out_proj\n",
            "Initializing transformer.h.2.attn.out_proj\n",
            "Adding adapter to transformer.h.2.mlp.fc_in\n",
            "Initializing transformer.h.2.mlp.fc_in\n",
            "Adding adapter to transformer.h.2.mlp.fc_out\n",
            "Initializing transformer.h.2.mlp.fc_out\n",
            "Adding adapter to transformer.h.3.attn.k_proj\n",
            "Initializing transformer.h.3.attn.k_proj\n",
            "Adding adapter to transformer.h.3.attn.v_proj\n",
            "Initializing transformer.h.3.attn.v_proj\n",
            "Adding adapter to transformer.h.3.attn.q_proj\n",
            "Initializing transformer.h.3.attn.q_proj\n",
            "Adding adapter to transformer.h.3.attn.out_proj\n",
            "Initializing transformer.h.3.attn.out_proj\n",
            "Adding adapter to transformer.h.3.mlp.fc_in\n",
            "Initializing transformer.h.3.mlp.fc_in\n",
            "Adding adapter to transformer.h.3.mlp.fc_out\n",
            "Initializing transformer.h.3.mlp.fc_out\n",
            "Adding adapter to transformer.h.4.attn.k_proj\n",
            "Initializing transformer.h.4.attn.k_proj\n",
            "Adding adapter to transformer.h.4.attn.v_proj\n",
            "Initializing transformer.h.4.attn.v_proj\n",
            "Adding adapter to transformer.h.4.attn.q_proj\n",
            "Initializing transformer.h.4.attn.q_proj\n",
            "Adding adapter to transformer.h.4.attn.out_proj\n",
            "Initializing transformer.h.4.attn.out_proj\n",
            "Adding adapter to transformer.h.4.mlp.fc_in\n",
            "Initializing transformer.h.4.mlp.fc_in\n",
            "Adding adapter to transformer.h.4.mlp.fc_out\n",
            "Initializing transformer.h.4.mlp.fc_out\n",
            "Adding adapter to transformer.h.5.attn.k_proj\n",
            "Initializing transformer.h.5.attn.k_proj\n",
            "Adding adapter to transformer.h.5.attn.v_proj\n",
            "Initializing transformer.h.5.attn.v_proj\n",
            "Adding adapter to transformer.h.5.attn.q_proj\n",
            "Initializing transformer.h.5.attn.q_proj\n",
            "Adding adapter to transformer.h.5.attn.out_proj\n",
            "Initializing transformer.h.5.attn.out_proj\n",
            "Adding adapter to transformer.h.5.mlp.fc_in\n",
            "Initializing transformer.h.5.mlp.fc_in\n",
            "Adding adapter to transformer.h.5.mlp.fc_out\n",
            "Initializing transformer.h.5.mlp.fc_out\n",
            "Adding adapter to transformer.h.6.attn.k_proj\n",
            "Initializing transformer.h.6.attn.k_proj\n",
            "Adding adapter to transformer.h.6.attn.v_proj\n",
            "Initializing transformer.h.6.attn.v_proj\n",
            "Adding adapter to transformer.h.6.attn.q_proj\n",
            "Initializing transformer.h.6.attn.q_proj\n",
            "Adding adapter to transformer.h.6.attn.out_proj\n",
            "Initializing transformer.h.6.attn.out_proj\n",
            "Adding adapter to transformer.h.6.mlp.fc_in\n",
            "Initializing transformer.h.6.mlp.fc_in\n",
            "Adding adapter to transformer.h.6.mlp.fc_out\n",
            "Initializing transformer.h.6.mlp.fc_out\n",
            "Adding adapter to transformer.h.7.attn.k_proj\n",
            "Initializing transformer.h.7.attn.k_proj\n",
            "Adding adapter to transformer.h.7.attn.v_proj\n",
            "Initializing transformer.h.7.attn.v_proj\n",
            "Adding adapter to transformer.h.7.attn.q_proj\n",
            "Initializing transformer.h.7.attn.q_proj\n",
            "Adding adapter to transformer.h.7.attn.out_proj\n",
            "Initializing transformer.h.7.attn.out_proj\n",
            "Adding adapter to transformer.h.7.mlp.fc_in\n",
            "Initializing transformer.h.7.mlp.fc_in\n",
            "Adding adapter to transformer.h.7.mlp.fc_out\n",
            "Initializing transformer.h.7.mlp.fc_out\n",
            "Adding adapter to transformer.h.8.attn.k_proj\n",
            "Initializing transformer.h.8.attn.k_proj\n",
            "Adding adapter to transformer.h.8.attn.v_proj\n",
            "Initializing transformer.h.8.attn.v_proj\n",
            "Adding adapter to transformer.h.8.attn.q_proj\n",
            "Initializing transformer.h.8.attn.q_proj\n",
            "Adding adapter to transformer.h.8.attn.out_proj\n",
            "Initializing transformer.h.8.attn.out_proj\n",
            "Adding adapter to transformer.h.8.mlp.fc_in\n",
            "Initializing transformer.h.8.mlp.fc_in\n",
            "Adding adapter to transformer.h.8.mlp.fc_out\n",
            "Initializing transformer.h.8.mlp.fc_out\n",
            "Adding adapter to transformer.h.9.attn.k_proj\n",
            "Initializing transformer.h.9.attn.k_proj\n",
            "Adding adapter to transformer.h.9.attn.v_proj\n",
            "Initializing transformer.h.9.attn.v_proj\n",
            "Adding adapter to transformer.h.9.attn.q_proj\n",
            "Initializing transformer.h.9.attn.q_proj\n",
            "Adding adapter to transformer.h.9.attn.out_proj\n",
            "Initializing transformer.h.9.attn.out_proj\n",
            "Adding adapter to transformer.h.9.mlp.fc_in\n",
            "Initializing transformer.h.9.mlp.fc_in\n",
            "Adding adapter to transformer.h.9.mlp.fc_out\n",
            "Initializing transformer.h.9.mlp.fc_out\n",
            "Adding adapter to transformer.h.10.attn.k_proj\n",
            "Initializing transformer.h.10.attn.k_proj\n",
            "Adding adapter to transformer.h.10.attn.v_proj\n",
            "Initializing transformer.h.10.attn.v_proj\n",
            "Adding adapter to transformer.h.10.attn.q_proj\n",
            "Initializing transformer.h.10.attn.q_proj\n",
            "Adding adapter to transformer.h.10.attn.out_proj\n",
            "Initializing transformer.h.10.attn.out_proj\n",
            "Adding adapter to transformer.h.10.mlp.fc_in\n",
            "Initializing transformer.h.10.mlp.fc_in\n",
            "Adding adapter to transformer.h.10.mlp.fc_out\n",
            "Initializing transformer.h.10.mlp.fc_out\n",
            "Adding adapter to transformer.h.11.attn.k_proj\n",
            "Initializing transformer.h.11.attn.k_proj\n",
            "Adding adapter to transformer.h.11.attn.v_proj\n",
            "Initializing transformer.h.11.attn.v_proj\n",
            "Adding adapter to transformer.h.11.attn.q_proj\n",
            "Initializing transformer.h.11.attn.q_proj\n",
            "Adding adapter to transformer.h.11.attn.out_proj\n",
            "Initializing transformer.h.11.attn.out_proj\n",
            "Adding adapter to transformer.h.11.mlp.fc_in\n",
            "Initializing transformer.h.11.mlp.fc_in\n",
            "Adding adapter to transformer.h.11.mlp.fc_out\n",
            "Initializing transformer.h.11.mlp.fc_out\n",
            "Adding adapter to transformer.h.12.attn.k_proj\n",
            "Initializing transformer.h.12.attn.k_proj\n",
            "Adding adapter to transformer.h.12.attn.v_proj\n",
            "Initializing transformer.h.12.attn.v_proj\n",
            "Adding adapter to transformer.h.12.attn.q_proj\n",
            "Initializing transformer.h.12.attn.q_proj\n",
            "Adding adapter to transformer.h.12.attn.out_proj\n",
            "Initializing transformer.h.12.attn.out_proj\n",
            "Adding adapter to transformer.h.12.mlp.fc_in\n",
            "Initializing transformer.h.12.mlp.fc_in\n",
            "Adding adapter to transformer.h.12.mlp.fc_out\n",
            "Initializing transformer.h.12.mlp.fc_out\n",
            "Adding adapter to transformer.h.13.attn.k_proj\n",
            "Initializing transformer.h.13.attn.k_proj\n",
            "Adding adapter to transformer.h.13.attn.v_proj\n",
            "Initializing transformer.h.13.attn.v_proj\n",
            "Adding adapter to transformer.h.13.attn.q_proj\n",
            "Initializing transformer.h.13.attn.q_proj\n",
            "Adding adapter to transformer.h.13.attn.out_proj\n",
            "Initializing transformer.h.13.attn.out_proj\n",
            "Adding adapter to transformer.h.13.mlp.fc_in\n",
            "Initializing transformer.h.13.mlp.fc_in\n",
            "Adding adapter to transformer.h.13.mlp.fc_out\n",
            "Initializing transformer.h.13.mlp.fc_out\n",
            "Adding adapter to transformer.h.14.attn.k_proj\n",
            "Initializing transformer.h.14.attn.k_proj\n",
            "Adding adapter to transformer.h.14.attn.v_proj\n",
            "Initializing transformer.h.14.attn.v_proj\n",
            "Adding adapter to transformer.h.14.attn.q_proj\n",
            "Initializing transformer.h.14.attn.q_proj\n",
            "Adding adapter to transformer.h.14.attn.out_proj\n",
            "Initializing transformer.h.14.attn.out_proj\n",
            "Adding adapter to transformer.h.14.mlp.fc_in\n",
            "Initializing transformer.h.14.mlp.fc_in\n",
            "Adding adapter to transformer.h.14.mlp.fc_out\n",
            "Initializing transformer.h.14.mlp.fc_out\n",
            "Adding adapter to transformer.h.15.attn.k_proj\n",
            "Initializing transformer.h.15.attn.k_proj\n",
            "Adding adapter to transformer.h.15.attn.v_proj\n",
            "Initializing transformer.h.15.attn.v_proj\n",
            "Adding adapter to transformer.h.15.attn.q_proj\n",
            "Initializing transformer.h.15.attn.q_proj\n",
            "Adding adapter to transformer.h.15.attn.out_proj\n",
            "Initializing transformer.h.15.attn.out_proj\n",
            "Adding adapter to transformer.h.15.mlp.fc_in\n",
            "Initializing transformer.h.15.mlp.fc_in\n",
            "Adding adapter to transformer.h.15.mlp.fc_out\n",
            "Initializing transformer.h.15.mlp.fc_out\n",
            "Adding adapter to transformer.h.16.attn.k_proj\n",
            "Initializing transformer.h.16.attn.k_proj\n",
            "Adding adapter to transformer.h.16.attn.v_proj\n",
            "Initializing transformer.h.16.attn.v_proj\n",
            "Adding adapter to transformer.h.16.attn.q_proj\n",
            "Initializing transformer.h.16.attn.q_proj\n",
            "Adding adapter to transformer.h.16.attn.out_proj\n",
            "Initializing transformer.h.16.attn.out_proj\n",
            "Adding adapter to transformer.h.16.mlp.fc_in\n",
            "Initializing transformer.h.16.mlp.fc_in\n",
            "Adding adapter to transformer.h.16.mlp.fc_out\n",
            "Initializing transformer.h.16.mlp.fc_out\n",
            "Adding adapter to transformer.h.17.attn.k_proj\n",
            "Initializing transformer.h.17.attn.k_proj\n",
            "Adding adapter to transformer.h.17.attn.v_proj\n",
            "Initializing transformer.h.17.attn.v_proj\n",
            "Adding adapter to transformer.h.17.attn.q_proj\n",
            "Initializing transformer.h.17.attn.q_proj\n",
            "Adding adapter to transformer.h.17.attn.out_proj\n",
            "Initializing transformer.h.17.attn.out_proj\n",
            "Adding adapter to transformer.h.17.mlp.fc_in\n",
            "Initializing transformer.h.17.mlp.fc_in\n",
            "Adding adapter to transformer.h.17.mlp.fc_out\n",
            "Initializing transformer.h.17.mlp.fc_out\n",
            "Adding adapter to transformer.h.18.attn.k_proj\n",
            "Initializing transformer.h.18.attn.k_proj\n",
            "Adding adapter to transformer.h.18.attn.v_proj\n",
            "Initializing transformer.h.18.attn.v_proj\n",
            "Adding adapter to transformer.h.18.attn.q_proj\n",
            "Initializing transformer.h.18.attn.q_proj\n",
            "Adding adapter to transformer.h.18.attn.out_proj\n",
            "Initializing transformer.h.18.attn.out_proj\n",
            "Adding adapter to transformer.h.18.mlp.fc_in\n",
            "Initializing transformer.h.18.mlp.fc_in\n",
            "Adding adapter to transformer.h.18.mlp.fc_out\n",
            "Initializing transformer.h.18.mlp.fc_out\n",
            "Adding adapter to transformer.h.19.attn.k_proj\n",
            "Initializing transformer.h.19.attn.k_proj\n",
            "Adding adapter to transformer.h.19.attn.v_proj\n",
            "Initializing transformer.h.19.attn.v_proj\n",
            "Adding adapter to transformer.h.19.attn.q_proj\n",
            "Initializing transformer.h.19.attn.q_proj\n",
            "Adding adapter to transformer.h.19.attn.out_proj\n",
            "Initializing transformer.h.19.attn.out_proj\n",
            "Adding adapter to transformer.h.19.mlp.fc_in\n",
            "Initializing transformer.h.19.mlp.fc_in\n",
            "Adding adapter to transformer.h.19.mlp.fc_out\n",
            "Initializing transformer.h.19.mlp.fc_out\n",
            "Adding adapter to transformer.h.20.attn.k_proj\n",
            "Initializing transformer.h.20.attn.k_proj\n",
            "Adding adapter to transformer.h.20.attn.v_proj\n",
            "Initializing transformer.h.20.attn.v_proj\n",
            "Adding adapter to transformer.h.20.attn.q_proj\n",
            "Initializing transformer.h.20.attn.q_proj\n",
            "Adding adapter to transformer.h.20.attn.out_proj\n",
            "Initializing transformer.h.20.attn.out_proj\n",
            "Adding adapter to transformer.h.20.mlp.fc_in\n",
            "Initializing transformer.h.20.mlp.fc_in\n",
            "Adding adapter to transformer.h.20.mlp.fc_out\n",
            "Initializing transformer.h.20.mlp.fc_out\n",
            "Adding adapter to transformer.h.21.attn.k_proj\n",
            "Initializing transformer.h.21.attn.k_proj\n",
            "Adding adapter to transformer.h.21.attn.v_proj\n",
            "Initializing transformer.h.21.attn.v_proj\n",
            "Adding adapter to transformer.h.21.attn.q_proj\n",
            "Initializing transformer.h.21.attn.q_proj\n",
            "Adding adapter to transformer.h.21.attn.out_proj\n",
            "Initializing transformer.h.21.attn.out_proj\n",
            "Adding adapter to transformer.h.21.mlp.fc_in\n",
            "Initializing transformer.h.21.mlp.fc_in\n",
            "Adding adapter to transformer.h.21.mlp.fc_out\n",
            "Initializing transformer.h.21.mlp.fc_out\n",
            "Adding adapter to transformer.h.22.attn.k_proj\n",
            "Initializing transformer.h.22.attn.k_proj\n",
            "Adding adapter to transformer.h.22.attn.v_proj\n",
            "Initializing transformer.h.22.attn.v_proj\n",
            "Adding adapter to transformer.h.22.attn.q_proj\n",
            "Initializing transformer.h.22.attn.q_proj\n",
            "Adding adapter to transformer.h.22.attn.out_proj\n",
            "Initializing transformer.h.22.attn.out_proj\n",
            "Adding adapter to transformer.h.22.mlp.fc_in\n",
            "Initializing transformer.h.22.mlp.fc_in\n",
            "Adding adapter to transformer.h.22.mlp.fc_out\n",
            "Initializing transformer.h.22.mlp.fc_out\n",
            "Adding adapter to transformer.h.23.attn.k_proj\n",
            "Initializing transformer.h.23.attn.k_proj\n",
            "Adding adapter to transformer.h.23.attn.v_proj\n",
            "Initializing transformer.h.23.attn.v_proj\n",
            "Adding adapter to transformer.h.23.attn.q_proj\n",
            "Initializing transformer.h.23.attn.q_proj\n",
            "Adding adapter to transformer.h.23.attn.out_proj\n",
            "Initializing transformer.h.23.attn.out_proj\n",
            "Adding adapter to transformer.h.23.mlp.fc_in\n",
            "Initializing transformer.h.23.mlp.fc_in\n",
            "Adding adapter to transformer.h.23.mlp.fc_out\n",
            "Initializing transformer.h.23.mlp.fc_out\n",
            "Adding adapter to transformer.h.24.attn.k_proj\n",
            "Initializing transformer.h.24.attn.k_proj\n",
            "Adding adapter to transformer.h.24.attn.v_proj\n",
            "Initializing transformer.h.24.attn.v_proj\n",
            "Adding adapter to transformer.h.24.attn.q_proj\n",
            "Initializing transformer.h.24.attn.q_proj\n",
            "Adding adapter to transformer.h.24.attn.out_proj\n",
            "Initializing transformer.h.24.attn.out_proj\n",
            "Adding adapter to transformer.h.24.mlp.fc_in\n",
            "Initializing transformer.h.24.mlp.fc_in\n",
            "Adding adapter to transformer.h.24.mlp.fc_out\n",
            "Initializing transformer.h.24.mlp.fc_out\n",
            "Adding adapter to transformer.h.25.attn.k_proj\n",
            "Initializing transformer.h.25.attn.k_proj\n",
            "Adding adapter to transformer.h.25.attn.v_proj\n",
            "Initializing transformer.h.25.attn.v_proj\n",
            "Adding adapter to transformer.h.25.attn.q_proj\n",
            "Initializing transformer.h.25.attn.q_proj\n",
            "Adding adapter to transformer.h.25.attn.out_proj\n",
            "Initializing transformer.h.25.attn.out_proj\n",
            "Adding adapter to transformer.h.25.mlp.fc_in\n",
            "Initializing transformer.h.25.mlp.fc_in\n",
            "Adding adapter to transformer.h.25.mlp.fc_out\n",
            "Initializing transformer.h.25.mlp.fc_out\n",
            "Adding adapter to transformer.h.26.attn.k_proj\n",
            "Initializing transformer.h.26.attn.k_proj\n",
            "Adding adapter to transformer.h.26.attn.v_proj\n",
            "Initializing transformer.h.26.attn.v_proj\n",
            "Adding adapter to transformer.h.26.attn.q_proj\n",
            "Initializing transformer.h.26.attn.q_proj\n",
            "Adding adapter to transformer.h.26.attn.out_proj\n",
            "Initializing transformer.h.26.attn.out_proj\n",
            "Adding adapter to transformer.h.26.mlp.fc_in\n",
            "Initializing transformer.h.26.mlp.fc_in\n",
            "Adding adapter to transformer.h.26.mlp.fc_out\n",
            "Initializing transformer.h.26.mlp.fc_out\n",
            "Adding adapter to transformer.h.27.attn.k_proj\n",
            "Initializing transformer.h.27.attn.k_proj\n",
            "Adding adapter to transformer.h.27.attn.v_proj\n",
            "Initializing transformer.h.27.attn.v_proj\n",
            "Adding adapter to transformer.h.27.attn.q_proj\n",
            "Initializing transformer.h.27.attn.q_proj\n",
            "Adding adapter to transformer.h.27.attn.out_proj\n",
            "Initializing transformer.h.27.attn.out_proj\n",
            "Adding adapter to transformer.h.27.mlp.fc_in\n",
            "Initializing transformer.h.27.mlp.fc_in\n",
            "Adding adapter to transformer.h.27.mlp.fc_out\n",
            "Initializing transformer.h.27.mlp.fc_out\n",
            "Adding adapter to lm_head\n",
            "Initializing lm_head\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPTJForCausalLM(\n",
              "  (transformer): GPTJModel(\n",
              "    (wte): FrozenBNBEmbedding(50400, 4096)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-27): 28 x GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
              "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): FrozenBNBLinear(4096, 50400)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def add_adapters(model, adapter_dim=4, p = 0.1):\n",
        "    assert adapter_dim > 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "      if isinstance(module, FrozenBNBLinear):\n",
        "          if \"attn\" in name or \"mlp\" in name or \"head\" in name:\n",
        "              print(\"Adding adapter to\", name)\n",
        "              module.adapter = nn.Sequential(\n",
        "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
        "                nn.Dropout(p=p),\n",
        "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
        "            )\n",
        "              print(\"Initializing\", name)\n",
        "              nn.init.zeros_(module.adapter[2].weight)\n",
        "\n",
        "          else:\n",
        "              print(\"Not adding adapter to\", name)\n",
        "      elif isinstance(module, FrozenBNBEmbedding):\n",
        "          print(\"Adding adapter to\", name)\n",
        "          module.adapter = nn.Sequential(\n",
        "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
        "                nn.Dropout(p=p),\n",
        "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
        "            )\n",
        "          print(\"Initializing\", name)\n",
        "          nn.init.zeros_(module.adapter[2].weight)\n",
        "\n",
        "add_adapters(gpt)\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:07.895447Z",
          "iopub.status.busy": "2023-04-09T16:49:07.894550Z",
          "iopub.status.idle": "2023-04-09T16:49:08.009610Z",
          "shell.execute_reply": "2023-04-09T16:49:08.008480Z",
          "shell.execute_reply.started": "2023-04-09T16:49:07.895411Z"
        },
        "id": "9DXh5aTVw19M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from bitsandbytes.optim import Adam8bit\n",
        "\n",
        "# enable gradient checkpointing\n",
        "gpt.gradient_checkpointing_enable()\n",
        "#optimizer is initialized with a learning rate of 1e-5 and a weight decay of 0.01\n",
        "optimizer = Adam8bit(gpt.parameters(), lr=1e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:08.011695Z",
          "iopub.status.busy": "2023-04-09T16:49:08.011255Z",
          "iopub.status.idle": "2023-04-09T16:49:08.016585Z",
          "shell.execute_reply": "2023-04-09T16:49:08.015488Z",
          "shell.execute_reply.started": "2023-04-09T16:49:08.011662Z"
        },
        "id": "5GNk6yCEw19M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGnuo7YX4qG4"
      },
      "source": [
        "## **Initializing a learning rate scheduler that gradually increases the learning rate from 0 to its maximum value, then keeps it constant until the end of training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:08.018863Z",
          "iopub.status.busy": "2023-04-09T16:49:08.018388Z",
          "iopub.status.idle": "2023-04-09T16:49:08.032035Z",
          "shell.execute_reply": "2023-04-09T16:49:08.030895Z",
          "shell.execute_reply.started": "2023-04-09T16:49:08.018820Z"
        },
        "id": "djyiBdBTw19M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer, int(num_training_steps*0.1), num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:08.033930Z",
          "iopub.status.busy": "2023-04-09T16:49:08.033372Z",
          "iopub.status.idle": "2023-04-09T16:49:08.038948Z",
          "shell.execute_reply": "2023-04-09T16:49:08.037102Z",
          "shell.execute_reply.started": "2023-04-09T16:49:08.033898Z"
        },
        "id": "8TonoILcw19M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "filepath = '/kaggle/working/model.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVHilPUL4zCp"
      },
      "source": [
        "## **Training a GPT model using PyTorch and the Hugging Face Transformers library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "cb9985c8c4ea4c99add165b763e7c8e3",
            "f40db0c5bbb84644953bb66830130c93"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-04-09T16:49:08.042304Z",
          "iopub.status.busy": "2023-04-09T16:49:08.041289Z",
          "iopub.status.idle": "2023-04-09T16:54:40.404325Z",
          "shell.execute_reply": "2023-04-09T16:54:40.403327Z",
          "shell.execute_reply.started": "2023-04-09T16:49:08.042245Z"
        },
        "id": "tEYAyIZMw19M",
        "outputId": "4887b11b-e385-4087-b6fa-ee73df995c92",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f40db0c5bbb84644953bb66830130c93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/90 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.2753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.2208, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.9448, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.0377, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.9986, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.1293, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(6.3937, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.2045, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.4952, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.3439, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.6936, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.2281, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.3033, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.2001, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.1796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.1793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.5999, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.1725, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.7563, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.9168, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.8263, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.9972, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(6.1360, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.9981, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.3321, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.1267, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.5081, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.0711, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.1451, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.0389, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.0222, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.8412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.9493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.4676, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.8495, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.4721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7582, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.6122, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.8447, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.8203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.7631, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.1524, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.8873, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.3067, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.9088, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.9860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.8694, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.8606, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.5783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.7134, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.3337, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.8391, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.6341, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.2074, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.6124, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.4198, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7076, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.5455, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.5648, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.9983, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.6869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.1380, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.8561, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7403, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7404, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.4037, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.5506, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.2415, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7302, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.4905, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.5199, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.2983, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.6219, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5.3863, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.4471, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.9143, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.5667, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.0498, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7173, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.7930, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.6771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(3.6811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.3233, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4.4802, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "gpt.train()\n",
        "gpt.gradient_checkpointing_enable()\n",
        "k = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        k = k + 1\n",
        "        if k % 500 == 0:\n",
        "\n",
        "          print(k)\n",
        "          state = {'k' : k, 'epoch': num_epochs, 'lr_scheduler': lr_scheduler.state_dict(), 'state_dict': gpt.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "          torch.save(state, filepath)\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        with torch.autograd.profiler.record_function(\"model_inference\"):\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                out = gpt.forward(**batch,)\n",
        "\n",
        "                loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n",
        "                                  reduction='mean', label_smoothing=0.1)\n",
        "\n",
        "        print(loss)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        progress_bar.update(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txI-wz0m47aq"
      },
      "source": [
        "## **Using GPT-J to generate text given a prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:54:40.406736Z",
          "iopub.status.busy": "2023-04-09T16:54:40.406105Z",
          "iopub.status.idle": "2023-04-09T16:55:18.075356Z",
          "shell.execute_reply": "2023-04-09T16:55:18.074341Z",
          "shell.execute_reply.started": "2023-04-09T16:54:40.406699Z"
        },
        "id": "UQXLXTK9w19N",
        "outputId": "d8c29b49-e3a2-4440-d5dd-9057c5acdf57",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**************************************************************\n",
            "[questions]:Quel est le taux de la retenue à la source à appliquer sur la TVA relative aux montants supérieurs à 1000 DT payés par les services de l’Etat à partir du 01/01/2016?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "GPT-J : [questions]:Quel est le taux de la retenue à la source à appliquer sur la TVA relative aux montants supérieurs à 1000 DT payés par les services de l’Etat à partir du 01/01/2016?\n",
            "comments.answers:Commentaires : Aucun commentaire...<|endoftext|>\n",
            "**************************************************************\n",
            "[questions]:Quelles sont les obligations relatives aux factures et aux titres de mouvement prévues par la législation fiscale?\n",
            "\n",
            "\n",
            "GPT-J : [questions]:Quelles sont les obligations relatives aux factures et aux titres de mouvement prévues par la législation fiscale?[/questions]\n",
            "\n",
            "I would like to know, in what case a customer is not required to pay any taxes during the payment of an installment contract? Example: The seller and buyer are under the same taxation jurisdiction; it seems there should be no taxes on this transaction ; does anybody has more informations about the subject or is their some document explaining the rights for tax evasion in such type situation.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "gpt.eval()\n",
        "for sentence in test.values:\n",
        "    print(\"**************************************************************\")\n",
        "    st = sentence.split('[answers]:')[0].strip()\n",
        "    print(st)\n",
        "    with torch.no_grad():\n",
        "        prompt = tokenizer(st, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "        prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "        out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "        print('\\n')\n",
        "        print(\"GPT-J :\" , tokenizer.decode(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:55:18.077837Z",
          "iopub.status.busy": "2023-04-09T16:55:18.077051Z",
          "iopub.status.idle": "2023-04-09T16:55:39.766354Z",
          "shell.execute_reply": "2023-04-09T16:55:39.765296Z",
          "shell.execute_reply.started": "2023-04-09T16:55:18.077787Z"
        },
        "id": "PAeBesG8w19N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.save(gpt.state_dict(), '/kaggle/working/gpt-j-6B.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:55:39.768940Z",
          "iopub.status.busy": "2023-04-09T16:55:39.767744Z",
          "iopub.status.idle": "2023-04-09T16:56:03.410991Z",
          "shell.execute_reply": "2023-04-09T16:56:03.409834Z",
          "shell.execute_reply.started": "2023-04-09T16:55:39.768911Z"
        },
        "id": "hdTo-WVzw19N",
        "outputId": "06ff394b-a85f-4467-f815-9ec0b5fb45c6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[questions]:Who is the health minister of Tunisia?\n",
            "==============================================================\n",
            "1 : Is it Mr.Driss Mhirsi?He was a physician who died in 2014.[link]http://enews7news.com/driss-mhiri... [question source: askapremedicineforme.com][hr]<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# Set the model to evaluation mode\n",
        "gpt.eval()\n",
        "# Generate text for the given prompt\n",
        "with torch.no_grad():\n",
        "  # Encode the prompt using the tokenizer and move it to the device\n",
        "  prompt = tokenizer(\"[questions]:Who is the health minister of Tunisia ?\", truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "  prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "  # Generate text using the GPT-J model\n",
        "  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "  # Decode the generated text using the tokenizer and print it\n",
        "  print(tokenizer.decode(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:56:03.413394Z",
          "iopub.status.busy": "2023-04-09T16:56:03.412711Z",
          "iopub.status.idle": "2023-04-09T16:56:30.138911Z",
          "shell.execute_reply": "2023-04-09T16:56:30.137904Z",
          "shell.execute_reply.started": "2023-04-09T16:56:03.413354Z"
        },
        "id": "JLgnE_-Sw19N",
        "outputId": "08367752-b7f6-40d2-c33e-a6db49d8a22a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[questions]:How does the tax office check??.The question is not about my situation but how it works?\n",
            "======Sale of a house with an existing loan = How to proceed?== The mortgage company said that i will be charged by 5% for every year in which we pay off debt and this amount can go upto 20%.\n",
            "I am paying already only half of the total price...<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "gpt.eval()\n",
        "with torch.no_grad():\n",
        "  prompt = tokenizer(\"[questions]:How does the tax office check??\", truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "  prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "  print(tokenizer.decode(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-09T16:56:30.141113Z",
          "iopub.status.busy": "2023-04-09T16:56:30.140502Z",
          "iopub.status.idle": "2023-04-09T16:56:38.222767Z",
          "shell.execute_reply": "2023-04-09T16:56:38.221701Z",
          "shell.execute_reply.started": "2023-04-09T16:56:30.141079Z"
        },
        "id": "0F7lYHrbw19N",
        "outputId": "fd3cf58e-9b05-4d16-dd41-8a09a6f0b8ba",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[questions]:What is influenza A(H1N1)?How does one contract it?\n",
            "Is there any cure for Influenza a (H1 N1)<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "gpt.eval()\n",
        "with torch.no_grad():\n",
        "  prompt = tokenizer(\"[questions]:What is influenza A(H1N1)?\", truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "  prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "  print(tokenizer.decode(out[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}